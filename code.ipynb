import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# Generate a synthetic dataset
np.random.seed(0)
n_samples = 1000

# Synthetic data
data = {
    'Age': np.random.randint(18, 70, n_samples),
    'Income': np.random.randint(20000, 100000, n_samples),
    'Bike_Type': np.random.choice(['Mountain', 'Road', 'Hybrid'], n_samples),
    'Price': np.random.randint(500, 5000, n_samples)
}

df = pd.DataFrame(data)

# Features and target variable
X = df[['Age', 'Income', 'Bike_Type']]
y = df['Price']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Preprocessing
numeric_features = ['Age', 'Income']
categorical_features = ['Bike_Type']

# Preprocessing pipeline
# Define preprocessing for numeric and categorical features
numeric_transformer = StandardScaler()
categorical_transformer = OneHotEncoder(handle_unknown='ignore')

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# Define the model pipeline
model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', RandomForestRegressor(n_estimators=100, random_state=0))
])

# Train the model
model.fit(X_train, y_train)

# Predict on the test set
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse:.2f}")

# Example of predicting a new sample
new_data = pd.DataFrame({
    'Age': [30],
    'Income': [60000],
    'Bike_Type': ['Mountain']
})

predicted_price = model.predict(new_data)
print(f"Predicted Price for new data: ${predicted_price[0]:.2f}")
